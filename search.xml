<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Centos7 Nat模式静态Ip配置</title>
    <url>/2020/Centos7%20Nat%E6%A8%A1%E5%BC%8F%E9%9D%99%E6%80%81Ip%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>虚拟机软件：VMware16</p>
<h3 id="一：修改虚拟网络编辑器"><a href="#一：修改虚拟网络编辑器" class="headerlink" title="一：修改虚拟网络编辑器"></a>一：修改虚拟网络编辑器</h3><p>1.进入虚拟网络编辑器，设置Vmnet8子网IP网段</p>
<img src="https://fastly.jsdelivr.net/gh/ccssbxf/img/blog/20220414201315.png" style="width: 60%;" />





<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220414203443.png" style="width: 60%;" />





<p>2.进入NAT设置，设置网关IP</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220414203512.png" style="width: 60%;" />

<h3 id="二：修改虚拟机网络配置文件"><a href="#二：修改虚拟机网络配置文件" class="headerlink" title="二：修改虚拟机网络配置文件"></a>二：修改虚拟机网络配置文件</h3><p>1.查看虚拟机使用的网络配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost /]# ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:94:37:5a brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.182.10/24 brd 192.168.10.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::8982:7848:648f:b491/64 scope link noprefixroute </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>说明使用的配置文件是ens33，当前ip为192.168.182.10</p>
<p>2.编辑相应的网络配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost /]# vi /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure>

<p>3.修改文件内容</p>
<p>注意等于号左右两边不能有空格</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">修改配置</span></span><br><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static   // 1.修改为静态模式</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes   // 2.开机自动启动网卡</span><br><span class="line">UUID=434ca492-505e-437f-b12a-6ff774c97f19</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">添加配置</span></span><br><span class="line">IPADDR=192.168.10.10   // static静态ip地址</span><br><span class="line">NETMASK=255.255.255.0    // 子网掩码（固定一致即可）</span><br><span class="line">GATEWAY=192.168.10.2    // 网关，与虚拟机VMnet8中设置的网关一致即可</span><br><span class="line">DNS1=114.114.114.114     // dns地址解析（固定一致即可）</span><br></pre></td></tr></table></figure>

<h3 id="三：重启网卡重新加载配置文件"><a href="#三：重启网卡重新加载配置文件" class="headerlink" title="三：重启网卡重新加载配置文件"></a>三：重启网卡重新加载配置文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost /]# systemctl restart network.service</span><br></pre></td></tr></table></figure>

<h3 id="四：验证网络"><a href="#四：验证网络" class="headerlink" title="四：验证网络"></a>四：验证网络</h3><p>1.查看当ip地址</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost /]# ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:94:37:5a brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.10.10/24 brd 192.168.10.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::8982:7848:648f:b491/64 scope link noprefixroute </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>2.验证网络是否通畅</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost /]# ping www.baidu.com</span><br><span class="line">PING www.a.shifen.com (36.152.44.96) 56(84) bytes of data.</span><br><span class="line">64 bytes from 36.152.44.96 (36.152.44.96): icmp_seq=1 ttl=128 time=23.0 ms</span><br><span class="line">64 bytes from 36.152.44.96 (36.152.44.96): icmp_seq=2 ttl=128 time=22.9 ms</span><br><span class="line">64 bytes from 36.152.44.96 (36.152.44.96): icmp_seq=3 ttl=128 time=23.2 ms</span><br><span class="line">64 bytes from 36.152.44.96 (36.152.44.96): icmp_seq=4 ttl=128 time=23.2 ms</span><br></pre></td></tr></table></figure>

<p>静态网络Ip配置完成</p>
<p>安装网络工具包，安装后可使用ifconfig命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost /]# yum -y install net-tools</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>静态IP</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell脚本配置集群免密登录</title>
    <url>/2022/Shell%E8%84%9A%E6%9C%AC%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/</url>
    <content><![CDATA[<p>使用时需要修改nodes和passwd</p>
<p>在集群的任意一台主机上，使用需要配置免密登录的用户执行该脚本，即可实现集群内所有主机之间的免密登录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">安装expect,用于自动交互任务</span></span><br><span class="line">sudo yum install -y expect</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">要设置免密登录的节点，第一台一定要为本机</span></span><br><span class="line">nodes=(hadoop101 hadoop102 hadoop103)</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">登录密码，所有节点一致</span></span><br><span class="line">passwd=123456</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">---本机免密登录其他节点---</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在所有节点生成秘钥</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果(<span class="built_in">yes</span>/no)?则自动选择<span class="built_in">yes</span>继续下一步，有的版本是(<span class="built_in">yes</span>/no/[fingerprint])?</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果password:则自动将passwd写在后面继续下一步</span></span><br><span class="line">create_key() &#123;</span><br><span class="line">  expect -c &quot;set timeout -1;</span><br><span class="line">        spawn ssh $USER@$node ssh-keygen -t rsa -P &#x27;&#x27; -f $HOME/.ssh/id_rsa;</span><br><span class="line">        expect &#123;</span><br><span class="line">                *(yes/no* &#123;send -- yes\r;exp_continue;&#125;</span><br><span class="line">                *password:* &#123;send -- $passwd\r;exp_continue;&#125;</span><br><span class="line">                eof\t&#123;exit 0&#125;</span><br><span class="line">        &#125;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ssh_create_key() &#123;</span><br><span class="line">  for node in $&#123;nodes[*]&#125;; do</span><br><span class="line">    create_key $node $passwd</span><br><span class="line">  done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ssh_create_key</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将本机公钥复制到所有节点</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果（<span class="built_in">yes</span>/no)?则自动选择<span class="built_in">yes</span>继续下一步，有的版本是(<span class="built_in">yes</span>/no/[fingerprint])?</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果password:则自动将passwd写在后面继续下一步</span></span><br><span class="line">copy_id() &#123;</span><br><span class="line">  expect -c &quot;set timeout -1;</span><br><span class="line">        spawn ssh-copy-id $USER@$node;</span><br><span class="line">        expect &#123;</span><br><span class="line">                *(yes/no* &#123;send -- yes\r;exp_continue;&#125;</span><br><span class="line">                *password:* &#123;send -- $passwd\r;exp_continue;&#125;</span><br><span class="line">                eof\t&#123;exit 0&#125;</span><br><span class="line">        &#125;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ssh_copy_id() &#123;</span><br><span class="line">  for node in $&#123;nodes[*]&#125;; do</span><br><span class="line">    copy_id $node $passwd</span><br><span class="line">  done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ssh_copy_id</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">---所有节点相互免密登录---</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将其他节点公钥追加到本机</span></span><br><span class="line">for node in $&#123;nodes[*]:1&#125;; do</span><br><span class="line">  ssh $USER@$node cat $HOME/.ssh/id_rsa.pub &gt;&gt;$HOME/.ssh/authorized_keys</span><br><span class="line">done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将包含所有节点公钥的文件复制到其他节点</span></span><br><span class="line">for node in $&#123;nodes[*]:1&#125;; do</span><br><span class="line">  scp $HOME/.ssh/authorized_keys $USER@$node:$HOME/.ssh/authorized_keys</span><br><span class="line">done</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将包含有所有节点连接信息的文件复制到其他节点</span></span><br><span class="line">for node in $&#123;nodes[*]:1&#125;; do</span><br><span class="line">  scp $HOME/.ssh/known_hosts $USER@$node:$HOME/.ssh/known_hosts</span><br><span class="line">done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">end</span></span><br></pre></td></tr></table></figure>

<p>注：配置免密登录的用户需要拥有sudo权限，用来安装expect。或者使用root用户安装expect后，注释掉第4行，再执行脚本。</p>
<p>参考：<a href="https://blog.csdn.net/ynzzxc/article/details/119999682">https://blog.csdn.net/ynzzxc/article/details/119999682</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>免密登录</tag>
      </tags>
  </entry>
  <entry>
    <title>Idea常用插件</title>
    <url>/2021/Idea%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>idea版本2021.1，有的插件在不同的idea版本下可能没有</p>
<h4 id="Alibaba-Java-Code-Guidelines"><a href="#Alibaba-Java-Code-Guidelines" class="headerlink" title="Alibaba Java Code Guidelines"></a>Alibaba Java Code Guidelines</h4><p>阿里巴巴代码规范检测。不符合代码规范的地方会有波浪线，鼠标移上去就会有相应的提示，有些问题甚至可以快速修复。</p>
<h4 id="Auto-filling-Java-call-arguments"><a href="#Auto-filling-Java-call-arguments" class="headerlink" title="Auto filling Java call arguments"></a>Auto filling Java call arguments</h4><p>调用一个函数，使用 Alt+Enter 组合键，调出 “Auto fill call parameters” 自动使用该函数定义的参数名填充。</p>
<h4 id="BashSupport-Pro"><a href="#BashSupport-Pro" class="headerlink" title="BashSupport Pro"></a>BashSupport Pro</h4><p>在idea里编写、运行shell脚本</p>
<h4 id="CamelCase"><a href="#CamelCase" class="headerlink" title="CamelCase"></a>CamelCase</h4><p>使用快捷键shift+alt+u，进行多种格式转换</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143410.png" style="width: 80%;" />



<h4 id="CodeGlance"><a href="#CodeGlance" class="headerlink" title="CodeGlance"></a>CodeGlance</h4><p>代码缩略图</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143448.png" style="width: 80%;" />



<h4 id="Codota"><a href="#Codota" class="headerlink" title="Codota"></a>Codota</h4><p>代码智能提示</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143510.png" style="width: 80%;" />

<p>并且可查看代码的例子</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143550.png" style="width: 80%;" />

<h4 id="GenerateAllSetter"><a href="#GenerateAllSetter" class="headerlink" title="GenerateAllSetter"></a>GenerateAllSetter</h4><p>通过快捷键alt+enter，自动调用所有 Setter 函数（可填充默认值）</p>
<h4 id="GenerateSerialVersionUID"><a href="#GenerateSerialVersionUID" class="headerlink" title="GenerateSerialVersionUID"></a>GenerateSerialVersionUID</h4><p>使用快捷键alt+insert，自动生成序列化 ID</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143623.png" style="width: 80%;" />

<h4 id="GsonFormatPlus"><a href="#GsonFormatPlus" class="headerlink" title="GsonFormatPlus"></a>GsonFormatPlus</h4><p>JSON字符串转实体类。新建一个实体类，使用快捷键alt+s调出插件画面</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143642.png" style="width: 80%;" />

<h4 id="IDE-Eval-Reset"><a href="#IDE-Eval-Reset" class="headerlink" title="IDE Eval Reset"></a>IDE Eval Reset</h4><p>idea以及其他可试用插件的破解工具，一直刷新试用期。先要在File-&gt;Settings-&gt;Plugins-&gt;设置的图标-&gt;Manage Plugin Repositories中添加仓库地址 <a href="https://plugins.zhile.io/">https://plugins.zhile.io</a></p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143704.png" style="width: 80%;" />

<h4 id="JRebel-and-XRebel-for-Intellij"><a href="#JRebel-and-XRebel-for-Intellij" class="headerlink" title="JRebel and XRebel for Intellij"></a>JRebel and XRebel for Intellij</h4><p>热部署插件，代码修改后按快捷键ctrl+f9编译一下代码即可生效</p>
<h4 id="LeetCode-Editor-Pro"><a href="#LeetCode-Editor-Pro" class="headerlink" title="LeetCode Editor Pro"></a>LeetCode Editor Pro</h4><p>leetcode刷题插件</p>
<h4 id="Maven-Helper"><a href="#Maven-Helper" class="headerlink" title="Maven Helper"></a>Maven Helper</h4><p>方便 maven 项目解决 jar 冲突，打开pom.xml文件，点击下方的</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143732.png" style="width: 80%;" />

<p>切换到此视图即可进行相应操作：</p>
<ol>
<li><p>Conflicts（查看冲突）</p>
</li>
<li><p>All Dependencies as List（列表形式查看所有依赖）</p>
</li>
<li><p>All Dependencies as Tree（树形式查看所有依赖）</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143752.png" style="width: 80%;" /></li>
</ol>
<h4 id="MyBatis-Log-Plugin"><a href="#MyBatis-Log-Plugin" class="headerlink" title="MyBatis Log Plugin"></a>MyBatis Log Plugin</h4><p>查看执行的sql语句</p>
<p>在配置文件中添加如下配置：</p>
<p>#org.apache.ibatis.logging.stdout.StdOutImpl 控制台打印 sql 语句方便调试 sql 语句执行错误<br>#org.apache.ibatis.logging.log4j2.Log4j2Impl 这个不在控制台打印查询结果，但是在 log4j 中打印<br># log-impl: org.apache.ibatis.logging.stdout.StdOutImpl<br>log-impl: org.apache.ibatis.logging.log4j2.Log4j2Impl</p>
<p>mybatis.configuration.log-impl&#x3D;org.apache.ibatis.logging.stdout.StdOutImpl</p>
<p>启动项目，只要控制台有sql语句打印，该插件都会将sql自动拼装打印出来，并且可以根据mapper.xml文件中定义的id来搜索</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143816.png" style="width: 80%;" />

<h4 id="MyBatisCodeHelperPro-Marketplace-Edition"><a href="#MyBatisCodeHelperPro-Marketplace-Edition" class="headerlink" title="MyBatisCodeHelperPro(Marketplace Edition)"></a>MyBatisCodeHelperPro(Marketplace Edition)</h4><p>使用教程：<a href="https://gejun123456.github.io/MyBatisCodeHelper-Pro">https://gejun123456.github.io/MyBatisCodeHelper-Pro</a></p>
<ul>
<li><strong>通过方法名 (不需要方法的返回值和参数 会自动推导出来) 来生成 sql 可以生成大部分单表操作的 sql 只需要一个方法的名字即可 会自动补全好方法的参数和返回值 和 springdatajpa 的语句基本一致</strong></li>
<li><strong>xml sql 几乎所有地方都有自动提示，sql 正确性检测，插件会识别 mybatis 的一系列标签 如 include trim set where，在这些标签之后的 sql 可以自动提示数据库的字段，检测 sql 的正确性，从此不用担心 sql 写错</strong></li>
<li><strong>直接从 Intellij 自带的数据库或者配置一个数据库生成 crud 代码 自动检测好 useGeneratedkey 自动配置好模块的文件夹 只用添加包名就可以生成代码了</strong></li>
<li><strong>xml 代码格式化</strong></li>
<li>从 java 类生成建表语句</li>
<li>数据库添加字段后可以继续生成，不会修改之前已经在接口或 xml 添加的自定义的方法 无需再去进行手动的添加</li>
<li>mybatis 接口和 xml 的互相跳转 支持一个 mybatis 接口对应多个 xml</li>
<li>mybatis 接口中的方法名重构支持</li>
<li>xml 中的 param 的自动提示 if test 的自动提示 resultMap refid 等的自动提示</li>
<li>resultMap 中的 property 的自动提示，检测，重构</li>
<li>resultMap 中 column 自动提示，检测</li>
<li>xml 中 refid，resultMap 等的跳转到定义</li>
<li>检测没有使用的 xml 可一键删除</li>
<li>检测 mybatis 接口中方法是否有实现，没有则报红 可创建一个空的 xml</li>
<li>mybatis 接口中一键添加 param 注解</li>
<li>mybatis 接口一键生成 xml</li>
<li>完整的 typeAlias 支持</li>
<li>param 检测 检测 #{ 中的内容是否有误</li>
<li>ognl 支持 if test when test foreach bind 中的自动补全，跳转和检测</li>
<li>支持 spring 将 mapper 注入到 spring 中 intellij 的 spring 注入不再报错 支持 springboot</li>
<li>一键生成 mybatis 接口的 testcase 无需启动 spring，复杂 sql 可进行快速测试</li>
<li>一键生成表关联的 join</li>
<li>一键从 sql 语句中 导出 resultMap</li>
</ul>
<h4 id="POJO-to-JSON"><a href="#POJO-to-JSON" class="headerlink" title="POJO to JSON"></a>POJO to JSON</h4><p>实体类转json，在类里右键</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143920.png" style="width: 80%;" />

<h4 id="RestfulTool"><a href="#RestfulTool" class="headerlink" title="RestfulTool"></a>RestfulTool</h4><p>找到项目里的接口，使用快捷键ctrl+alt+&#x2F;快速搜索接口</p>
<img src="https://gcore.jsdelivr.net/gh/ccssbxf/img/blog/20220430143938.png" style="width: 80%;" />

<h4 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h4><p>翻译</p>
<h4 id="Vue-js"><a href="#Vue-js" class="headerlink" title="Vue.js"></a>Vue.js</h4><p>开发前端vue使用</p>
<h4 id="Private-Notes"><a href="#Private-Notes" class="headerlink" title="Private Notes"></a>Private Notes</h4><p>添加注释， 按下Alt + Enter鼠标移出点击即可保存<br>已有私人注释 按下Alt + Enter即可快速编辑<br>Alt + p 可快速添加或者编辑私人注释<br>Alt + o 展示私人注释的其它操作<br>右键菜单私人注释查看操作</p>
]]></content>
      <categories>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH免密登录配置</title>
    <url>/2021/linux%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>1.在每台主机上执行以下命令生成公私钥（一路回车即可）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>2.发送公钥到集群所有主机（包括本机）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -p 22 -i ~/.ssh/id_rsa.pub 用户名@ip或主机名称</span><br></pre></td></tr></table></figure>

<p>3.当ssh端口非22端口时</p>
<p>需要将步骤2中的端口22更改为目标端口，并且需要执行以下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;Port 目标端口&quot; &gt;&gt;  ~/.ssh/config</span><br></pre></td></tr></table></figure>

<p>4.验证免密登录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh ip或者主机名称</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>免密登录</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase在线动态新增节点、删除节点</title>
    <url>/2022/HBase%E5%9C%A8%E7%BA%BF%E5%8A%A8%E6%80%81%E6%96%B0%E5%A2%9E%E8%8A%82%E7%82%B9%E3%80%81%E5%88%A0%E9%99%A4%E8%8A%82%E7%82%B9/</url>
    <content><![CDATA[<h2 id="一：新增节点"><a href="#一：新增节点" class="headerlink" title="一：新增节点"></a>一：新增节点</h2><p>注：HBase新增节点需要建立在这个节点已经有Hadoop的情况下</p>
<p>1.拷贝集群原有节点的HBase包到新节点相同路径下</p>
<p>2.修改&#x2F;home&#x2F;zhouhj&#x2F;app&#x2F;hbase-2.3.6&#x2F;conf&#x2F;regionservers配置文件，加入新节点，将该文件分发到所有的HBase节点上</p>
<p>3.在新节点启动HBase</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure>

<p>4.查看HBase集群状态</p>
<p>进入hbase shell，执行status查看状态，可以看到新节点已加入集群</p>
<p>5.数据平衡</p>
<p>hbase shell里执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">（开启负载均衡器，HBase会选择时机执行平衡数据）</span><br><span class="line">balance_switch true</span><br><span class="line">（下面这个是手动触发避免业务高峰期执行）</span><br><span class="line">balancer</span><br></pre></td></tr></table></figure>

<p>等命令执行完成，新加入的节点上已经有region，可以通过HBase管理页面看到</p>
<h2 id="二：删除节点"><a href="#二：删除节点" class="headerlink" title="二：删除节点"></a>二：删除节点</h2><p>1.在要下线的节点（比如我这里下线hadoop104），在hadoop104执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graceful_stop.sh hadoop104</span><br></pre></td></tr></table></figure>

<p>该命令会自动关闭 Load Balancer（banlance_switch），然后转移该 regionserver 维护的 region 到其他节点，将该节点关闭。除此之外，你还可以查看 remove 的过程，已经 assigned 了多少个 Region，还剩多少个 Region，每个 Region 的 Assigned 耗时，最终完成之后需要手动打开 load balancer [hbase shell 执行banlance_switch true]</p>
<p>2.删除&#x2F;home&#x2F;zhouhj&#x2F;app&#x2F;hbase-2.3.6&#x2F;conf&#x2F;regionservers配置文件里的相应节点</p>
<p>3.启用负载平衡器</p>
<p>hbase shell里执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">（开启负载均衡器，HBase会选择时机执行平衡数据）</span><br><span class="line">balance_switch true</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop在线动态新增节点、删除节点</title>
    <url>/2022/Hadoop%E5%9C%A8%E7%BA%BF%E5%8A%A8%E6%80%81%E6%96%B0%E5%A2%9E%E8%8A%82%E7%82%B9%E3%80%81%E5%88%A0%E9%99%A4%E8%8A%82%E7%82%B9/</url>
    <content><![CDATA[<h2 id="一：新增节点"><a href="#一：新增节点" class="headerlink" title="一：新增节点"></a>一：新增节点</h2><p>1.按原有数据节点配置一台新的数据节点（包括最初搭建集群的系统参数修改、时钟同步等操作）</p>
<p>2.配置节点主机映射，比如新增hadoop104节点，则在原有节点的&#x2F;etc&#x2F;hosts文件里添加映射信息，并将该文件分发到新节点</p>
<p>3.将新节点与集群内原有节点做免密登录</p>
<p>4.拷贝原数据节点上的环境变量配置、JDK、Hadoop安装包等到新节点相同路径下</p>
<p>5.在namenode修改&#x2F;home&#x2F;zhouhj&#x2F;app&#x2F;hadoop-3.2.2&#x2F;etc&#x2F;hadoop&#x2F;workers文件，新增hadoop104节点，并分发到所有节点上</p>
<p>6.查看hdfs-site.xml配置项dfs.datanode.data.dir的值，该值为数据节点存储数据的目录，如果新节点的这些目录有数据，将其清理干净</p>
<p>7.启动新节点上的进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start datanode</span><br><span class="line">yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure>

<p>8.刷新节点配置，让新节点能被集群感知到</p>
<p>在namenode执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<p>9.可以在hadoop管理页面或者使用命令hdfs dfsadmin -report，查看新节点是否加入hdfs集群，使用yarn node -list查看新节点是否加入yarn集群</p>
<p>10.平衡节点数据</p>
<p>在namenode节点执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#对hdfs负载设置均衡，因为默认的数据传输带宽比较低，可以设置为64M  </span><br><span class="line">hdfs dfsadmin -setBalancerBandwidth 67108864 </span><br><span class="line"></span><br><span class="line">#默认balancer的threshold为10%，即各个节点与集群总的存储使用率相差不超过10%，我们可将其设置为5%</span><br><span class="line">start-balancer.sh -threshold 5</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>1）如果不 balance，那么 cluster 会把新的数据都存放在新的 node 上，这样会降低 mapred 的工作效率<br>2）设置平衡阈值，默认是 10%，值越低各节点越平衡，但消耗时间也更长<br>3）设置 balance 的带宽，默认只有 1M&#x2F;s</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;64m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>11.通过Hadoop管理页面或者hdfs dfsadmin -report命令，查看数据迁移情况</p>
<h2 id="二：删除节点"><a href="#二：删除节点" class="headerlink" title="二：删除节点"></a>二：删除节点</h2><p>注：如果节点想要动态正常下线，除非预先在hdfs-site.xml文件配置了dfs.hosts.exclude，以及yarn-site.xml配置了yarn.resourcemanager.nodes.exclude-path 。否则就只能采用直接在要下线的数据节点进行停止进程，让集群认为这台数据节点进程掉了，自动进行副本转移（采用停进程的方式不能一次性下3台及以上，最好一台台操作）。</p>
<p>1.在dfs.hosts.exclude配置的路径的文件里，添加要下线的机器，一行一个</p>
<p>比如我的配置项</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">　　&lt;!--dfs.hosts.exclude定义的文件内容为,每个需要下线的机器，一行一个--&gt;</span><br><span class="line">　　&lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">　　&lt;value&gt;/home/zhouhj/app/hadoop-3.2.2/etc/hadoop/excludes&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>2.刷新节点配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<p>3.观察集群情况</p>
<p>此时在Hadoop管理页面可以看到Decommissioning Nodes的数量为刚刚在excludes里添加的数量，等副本迁移完成之后，这些节点状态会变成Decommissioned。或者使用hadoop dfsadmin -report以及yarn node -list查看，也能看到相关信息</p>
<p>4.待观察到节点状态变成Decommissioned之后，就可以停这台节点上的进程了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs --daemon stop datanode</span><br><span class="line">yarn --daemon stop nodemanager</span><br></pre></td></tr></table></figure>

<p>5.在&#x2F;home&#x2F;zhouhj&#x2F;app&#x2F;hadoop-3.2.2&#x2F;etc&#x2F;hadoop&#x2F;workers中删除对应节点</p>
<h2 id="三：重新加入删除的节点"><a href="#三：重新加入删除的节点" class="headerlink" title="三：重新加入删除的节点"></a>三：重新加入删除的节点</h2><p>1.在excludes文件中删除相关节点</p>
<p>2.在&#x2F;home&#x2F;zhouhj&#x2F;app&#x2F;hadoop-3.2.2&#x2F;etc&#x2F;hadoop&#x2F;workers中加入对应节点</p>
<p>3.在节点上启动进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start datanode</span><br><span class="line">yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure>

<p>4.刷新节点配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<p>5.平衡节点数据</p>
<p>参考新增节点平衡数据</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop HA集群两个NameNode都是standby状态</title>
    <url>/2022/Hadoop%20HA%E9%9B%86%E7%BE%A4%E4%B8%A4%E4%B8%AANameNode%E9%83%BD%E6%98%AFstandby%E7%8A%B6%E6%80%81/</url>
    <content><![CDATA[<p>1.查看zkfc进程是否正常</p>
<p>HadoopNameNode的主备选举由zkfc进程进行，如果zkfc进程挂了，则会导致两个NameNode都是standby的状态</p>
<p>启动zkfc进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start zkfc</span><br></pre></td></tr></table></figure>

<p>2.重新初始化zk里Hadoop的数据</p>
<p>在想成为active的那台namenode节点执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">初始化zk数据</span><br><span class="line">hdfs zkfc -formatZK</span><br><span class="line"></span><br><span class="line">重启hdfs集群</span><br><span class="line">stop-dfs.sh</span><br><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>class文件打成jar包</title>
    <url>/2023/class%E6%96%87%E4%BB%B6%E6%89%93%E6%88%90jar%E5%8C%85/</url>
    <content><![CDATA[<p>由于有时候的特殊需要，要将几个class文件单独打成jar包</p>
<p>1.按class文件的package建好相应的目录，比如原有class是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package com.zhouhj.tools.phoenix.udf;</span><br></pre></td></tr></table></figure>

<p>则创建从com开始到udf结束的目录</p>
<p>2.将class文件放到对应目录下</p>
<p>3.在com目录同级别的目录执行以下命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">jar -cvf phoenix-udf.jar com/</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>打包</tag>
      </tags>
  </entry>
  <entry>
    <title>Phoenix自定义函数UDF</title>
    <url>/2023/Phoenix%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0UDF/</url>
    <content><![CDATA[<h2 id="以自定义函数生成rowkey前缀为例"><a href="#以自定义函数生成rowkey前缀为例" class="headerlink" title="以自定义函数生成rowkey前缀为例"></a>以自定义函数生成rowkey前缀为例</h2><h3 id="一：依赖"><a href="#一：依赖" class="headerlink" title="一：依赖"></a>一：依赖</h3><p>要自定义phoenix函数，必须有以下依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--对应的phoenix的版本号--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="二：代码"><a href="#二：代码" class="headerlink" title="二：代码"></a>二：代码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhouhj.tools.phoenix.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.expression.Expression;</span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.expression.function.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.parse.FunctionParseNode;</span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.schema.tuple.Tuple;</span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.schema.types.PDataType;</span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.schema.types.PVarchar;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.security.MessageDigest;</span><br><span class="line"><span class="keyword">import</span> java.security.NoSuchAlgorithmException;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="meta">@FunctionParseNode</span>.BuiltInFunction(</span><br><span class="line">        name = <span class="string">&quot;rk&quot;</span>,</span><br><span class="line">        args = &#123;<span class="meta">@FunctionParseNode</span>.Argument(</span><br><span class="line">                allowedTypes = &#123;PVarchar.class&#125;</span><br><span class="line">        )&#125;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RowKeyFunction</span> <span class="keyword">extends</span> <span class="title class_">ScalarFunction</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">NAME</span> <span class="operator">=</span> <span class="string">&quot;rk&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">RowKeyFunction</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">RowKeyFunction</span><span class="params">(List&lt;Expression&gt; children)</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>(children);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getName</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> NAME;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">evaluate</span><span class="params">(Tuple tuple, ImmutableBytesWritable ptr)</span> &#123;</span><br><span class="line">        <span class="type">Expression</span> <span class="variable">idExp</span> <span class="operator">=</span> <span class="built_in">this</span>.getIdExpression();</span><br><span class="line">        <span class="keyword">if</span> (!idExp.evaluate(tuple, ptr)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">userId</span> <span class="operator">=</span> (String) PVarchar.INSTANCE.toObject(ptr, idExp.getSortOrder());</span><br><span class="line">            <span class="type">String</span> <span class="variable">rowKey</span> <span class="operator">=</span> <span class="built_in">this</span>.getRowKey(userId, <span class="string">&quot;%&quot;</span>);</span><br><span class="line">            ptr.set(PVarchar.INSTANCE.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> PDataType <span class="title function_">getDataType</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> PVarchar.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Expression <span class="title function_">getIdExpression</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.children.get(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String <span class="title function_">getRowKey</span><span class="params">(String userId, String def)</span> &#123;</span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        <span class="comment">//此处省略业务</span></span><br><span class="line">        <span class="keyword">return</span> sb.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="三：打包"><a href="#三：打包" class="headerlink" title="三：打包"></a>三：打包</h3><p>1.将自定义函数打成jar包</p>
<p>2.将自定义函数里用到的第三方的lib包也要拿出来（除HBase、Hadoop、Phoenix相关包以外）。比如额外引入了hutool且自定义函数里使用了的话，就需要hutool的lib包</p>
<h3 id="四：部署"><a href="#四：部署" class="headerlink" title="四：部署"></a>四：部署</h3><h4 id="1-配置修改"><a href="#1-配置修改" class="headerlink" title="1.配置修改"></a>1.配置修改</h4><p>修改hbase-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!-- phoenix支持自定义函数 -- &gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;phoenix.functions.allowUserDefinedFunctions&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 自定义函数，存储jar的hdfs目录 -- &gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.dynamic.jars.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;填写准备放自定义jar的地方，除了这个路径，jar放在其他路径哪怕指定了都没用。默认值是$&#123;hbase.rootdir&#125;/lib&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h4 id="2-包部署"><a href="#2-包部署" class="headerlink" title="2.包部署"></a>2.包部署</h4><p>将自定义的jar包和所依赖的jar包，放到hbase.dynamic.jars.dir配置的路径下</p>
<h4 id="3-函数注册"><a href="#3-函数注册" class="headerlink" title="3.函数注册"></a>3.函数注册</h4><p>进入phoenix客户端</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">注册永久函数</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> rk(<span class="type">varchar</span>) <span class="keyword">returns</span> <span class="type">varchar</span> <span class="keyword">as</span> <span class="string">&#x27;com.zhouhj.tools.phoenix.udf.RowKeyFunction&#x27;</span> <span class="keyword">using</span> jar <span class="string">&#x27;hdfs://mycluster/hbase/lib/PhoenixUDF.jar&#x27;</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> rk(<span class="type">varchar</span>) <span class="keyword">returns</span> <span class="type">varchar</span> <span class="keyword">as</span> <span class="string">&#x27;com.zhouhj.tools.phoenix.udf.RowKeyFunction&#x27;</span> <span class="keyword">using</span> jar <span class="string">&#x27;/hbase/lib/PhoenixUDF.jar&#x27;</span>;</span><br><span class="line"></span><br><span class="line">如果目录下没有其他包里的类会冲突的话，可以不需要指定jar</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> rk(<span class="type">varchar</span>) <span class="keyword">returns</span> <span class="type">varchar</span> <span class="keyword">as</span> <span class="string">&#x27;com.zhouhj.tools.phoenix.udf.RowKeyFunction&#x27;</span>;</span><br><span class="line"></span><br><span class="line">注册临时函数</span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">FUNCTION</span> ...</span><br><span class="line"></span><br><span class="line">如果注册路径错了，需要删除函数，使用以下语句</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">FUNCTION</span> IF <span class="keyword">EXISTS</span> rk;</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>1.如果使用jdbc连接，需要指定phoenix.functions.allowUserDefinedFunctions的值，例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">props.setProperty(<span class="string">&quot;phoenix.functions.allowUserDefinedFunctions&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"><span class="type">Connection</span> <span class="variable">conn</span> <span class="operator">=</span> DriverManager.getConnection(<span class="string">&quot;jdbc:phoenix:localhost&quot;</span>, props);</span><br></pre></td></tr></table></figure>

<p>2.如果是在程序中，如java、jdbc等使用到该函数，程序运行时，会将该jar复制到本地的hbase.local.dir路径下，默认是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.local.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;$&#123;hbase.tmp.dir&#125;/local/&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Directory on the local filesystem to be used</span><br><span class="line">    as a local storage.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>详见官网</p>
<p><a href="https://phoenix.apache.org/udf.html">https://phoenix.apache.org/udf.html</a></p>
<h4 id="4-使用函数"><a href="#4-使用函数" class="headerlink" title="4.使用函数"></a>4.使用函数</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:&gt; select rk(&#x27;abc&#x27;) as id;</span><br><span class="line">+---------+</span><br><span class="line">|   ID    |</span><br><span class="line">+---------+</span><br><span class="line">| 0593abc% |</span><br><span class="line">+---------+</span><br><span class="line">1 row selected (0.007 seconds)</span><br><span class="line">也可传入字段例如</span><br><span class="line">select * from xxx where id like rk(user_no);</span><br></pre></td></tr></table></figure>

<h4 id="5-补充说明"><a href="#5-补充说明" class="headerlink" title="5.补充说明"></a>5.补充说明</h4><p>1.自定义jar包只能放在hbase-site.xml里配置的hbase.dynamic.jars.dir路径下，放其他路径哪怕注册函数时使用using jar的语法指定了路径也不行</p>
<p>2.注册函数后，第一次使用自定义函数时，phoenix客户端会将hbase.dynamic.jars.dir路径下的所有jar包，下载到本地的hbase.local.dir路径作为本地缓存。后续如果Phoenix客户端重启，如果本地这个jar包存在的话，会从本地加载这个类到内存。所以，如果自定义函数有修改需要换包的话，这个路径下的包记得删除让他重新从hdfs上拉取。</p>
<p>3.查看有哪些自定义函数<br>select * from SYSTEM.”FUNCTION”;</p>
<p><img src="https://testingcf.jsdelivr.net/gh/ccssbxf/img@master/blog/20240428143156.png" alt="image-20240428143007559"></p>
]]></content>
      <categories>
        <category>Phoenix</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Phoenix</tag>
        <tag>UDF</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive自定义函数UDF、UDAF、UDTF</title>
    <url>/2024/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0UDF%E3%80%81UDAF%E3%80%81UDTF/</url>
    <content><![CDATA[<h2 id="自定义函数的种类"><a href="#自定义函数的种类" class="headerlink" title="自定义函数的种类"></a>自定义函数的种类</h2><p>UDF：一进一出，一对一的关系数据，例如substring、replace等</p>
<p>UDAF：多进一出，多对一的关系数据，例如sum、avg等</p>
<p>UDTF：一进多处，一对多的关系数据，例如collect_set、collect_list等</p>
<h2 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--对应的hive的版本号--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="UDF函数编写"><a href="#UDF函数编写" class="headerlink" title="UDF函数编写"></a>UDF函数编写</h2><h3 id="1-编写函数"><a href="#1-编写函数" class="headerlink" title="1.编写函数"></a>1.编写函数</h3><p>编写mysubstring(字段，开始位置，结束位置)为例，截取字符串</p>
<p>1.继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF类，有四个函数需要重写</p>
<p>2.代码块</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhouhj.tools.hive.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.NUMERIC_GROUP;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.STRING_GROUP;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> ZhouHJ</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySubstringUDF</span> <span class="keyword">extends</span> <span class="title class_">GenericUDF</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">transient</span> PrimitiveCategory[] inputTypes = <span class="keyword">new</span> <span class="title class_">PrimitiveCategory</span>[<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">transient</span> Converter[] converters = <span class="keyword">new</span> <span class="title class_">Converter</span>[<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">output</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//校验入参个数等初始化操作，返回类型为自定义函数返回值的类型，可参考GenericUDF类的子类的写法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ObjectInspector <span class="title function_">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException &#123;</span><br><span class="line">        <span class="comment">//校验参数列表长度</span></span><br><span class="line">        checkArgsSize(arguments, <span class="number">3</span>, <span class="number">3</span>);</span><br><span class="line">        <span class="comment">//用于检查函数的参数是否为原始类型（primitive）。</span></span><br><span class="line">        <span class="comment">// 它的作用是确保函数的参数是可以直接进行计算和操作的基本数据类型，而不是复杂的结构类型（如数组或结构体）。</span></span><br><span class="line">        checkArgPrimitive(arguments, <span class="number">0</span>);</span><br><span class="line">        checkArgPrimitive(arguments, <span class="number">1</span>);</span><br><span class="line">        checkArgPrimitive(arguments, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        checkArgGroups(arguments, <span class="number">0</span>, inputTypes, STRING_GROUP);</span><br><span class="line">        checkArgGroups(arguments, <span class="number">1</span>, inputTypes, NUMERIC_GROUP);</span><br><span class="line">        checkArgGroups(arguments, <span class="number">2</span>, inputTypes, NUMERIC_GROUP);</span><br><span class="line"></span><br><span class="line">        obtainStringConverter(arguments, <span class="number">0</span>, inputTypes, converters);</span><br><span class="line">        obtainIntConverter(arguments, <span class="number">1</span>, inputTypes, converters);</span><br><span class="line">        obtainIntConverter(arguments, <span class="number">2</span>, inputTypes, converters);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.writableStringObjectInspector;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//函数逻辑处理</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> getStringValue(arguments, <span class="number">0</span>, converters);</span><br><span class="line">        <span class="keyword">if</span> (StringUtils.isBlank(str)) &#123;</span><br><span class="line">            output.set(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> output;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//获取字符串的长度</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> str.length();</span><br><span class="line">        <span class="type">Integer</span> <span class="variable">startIndex</span> <span class="operator">=</span> getIntValue(arguments, <span class="number">1</span>, converters);</span><br><span class="line">        <span class="type">Integer</span> <span class="variable">endIndex</span> <span class="operator">=</span> getIntValue(arguments, <span class="number">2</span>, converters);</span><br><span class="line">        <span class="keyword">if</span> (startIndex &lt; <span class="number">0</span> || startIndex &gt;= len || startIndex &gt; endIndex) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentException</span>(<span class="string">&quot;startIndex is out of bound&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (endIndex &gt; len) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentException</span>(<span class="string">&quot;endIndex is out of bound&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">String</span> <span class="variable">substring</span> <span class="operator">=</span> str.substring(startIndex, endIndex);</span><br><span class="line">        output.set(substring);</span><br><span class="line">        <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//explain 执行计划中显示的语句,可不重写内容</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getDisplayString</span><span class="params">(String[] children)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> getStandardDisplayString(getFuncName(), children);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> String <span class="title function_">getFuncName</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;mysubstring&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="2-函数创建使用"><a href="#2-函数创建使用" class="headerlink" title="2.函数创建使用"></a>2.函数创建使用</h3><p>将打好的包上传到hdfs上，例如传到&#x2F;hive&#x2F;lib下</p>
<p>创建函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add</span> jar hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span>hive<span class="operator">/</span>lib<span class="operator">/</span>HiveUDF<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">function</span> mysubstring <span class="keyword">as</span> <span class="string">&#x27;com.zhouhj.tools.hive.udf.MySubstringUDF&#x27;</span> <span class="keyword">USING</span> JAR <span class="string">&#x27;hdfs:///hive/lib/HiveUDF-1.0-SNAPSHOT.jar&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>使用函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> mysubstring(<span class="string">&#x27;abc&#x27;</span>,<span class="number">0</span>,<span class="number">2</span>);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">ab</span><br></pre></td></tr></table></figure>



<h2 id="UDAF函数编写"><a href="#UDAF函数编写" class="headerlink" title="UDAF函数编写"></a>UDAF函数编写</h2><p>编写mysum(字段)为例，求和</p>
<p>Hive UDAF函数编写需要继承org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator类，有7个方法需要重写</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 确定各个阶段输入输出参数的数据格式ObjectInspectors  </span><br><span class="line">public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException;</span><br><span class="line">  </span><br><span class="line">// 保存数据聚集结果的类</span><br><span class="line">public abstract AggregationBuffer getNewAggregationBuffer() throws HiveException;</span><br><span class="line">  </span><br><span class="line">// 重置聚集结果</span><br><span class="line">public abstract void reset(AggregationBuffer agg) throws HiveException;</span><br><span class="line">  </span><br><span class="line">// map阶段，迭代处理输入sql传过来的列数据</span><br><span class="line">public abstract void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException;</span><br><span class="line">  </span><br><span class="line">// map与combiner结束返回结果，得到部分数据聚集结果  </span><br><span class="line">public abstract Object terminatePartial(AggregationBuffer agg) throws HiveException; </span><br><span class="line">  </span><br><span class="line">// combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。  </span><br><span class="line">public abstract void merge(AggregationBuffer agg, Object partial) throws HiveException;</span><br><span class="line">  </span><br><span class="line">// reducer阶段，输出最终结果  </span><br><span class="line">public abstract Object terminate(AggregationBuffer agg) throws HiveException;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>一般情况下，完整的 UDAF 逻辑是一个 mapreduce 过程，如果有 mapper 和 reducer，就会经历 PARTIAL1 (mapper)，FINAL (reducer)，如果还有 combiner，那就会经历 PARTIAL1 (mapper)，PARTIAL2 (combiner)，FINAL (reducer)。<br>而有一些情况下的 mapreduce，只有 mapper，而没有 reducer，所以就会只有 COMPLETE 阶段，这个阶段直接输入原始数据，出结果。</p>
<p>其中还涉及到一个Model的概念，Model代表了UDAF在MapReduce里的各个阶段</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">enum</span> <span class="title class_">Mode</span> &#123;  </span><br><span class="line">    <span class="comment">/** </span></span><br><span class="line"><span class="comment">     * PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合 </span></span><br><span class="line"><span class="comment">     * 将会调用iterate()和terminatePartial() </span></span><br><span class="line"><span class="comment">     */</span>  </span><br><span class="line">    PARTIAL1,  </span><br><span class="line">        <span class="comment">/** </span></span><br><span class="line"><span class="comment">     * PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合: </span></span><br><span class="line"><span class="comment">     * 将会调用merge() 和 terminatePartial()  </span></span><br><span class="line"><span class="comment">     */</span>  </span><br><span class="line">    PARTIAL2,  </span><br><span class="line">        <span class="comment">/** </span></span><br><span class="line"><span class="comment">     * FINAL: mapreduce的reduce阶段:从部分数据的聚合到完全聚合  </span></span><br><span class="line"><span class="comment">     * 将会调用merge()和terminate() </span></span><br><span class="line"><span class="comment">     */</span>  </span><br><span class="line">    FINAL,  </span><br><span class="line">        <span class="comment">/** </span></span><br><span class="line"><span class="comment">     * COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了:从原始数据直接到完全聚合 </span></span><br><span class="line"><span class="comment">      * 将会调用 iterate()和terminate() </span></span><br><span class="line"><span class="comment">     */</span>  </span><br><span class="line">    COMPLETE  </span><br><span class="line">  &#125;;  </span><br></pre></td></tr></table></figure>

<p>图解函数调用过程</p>
<img src="https://testingcf.jsdelivr.net/gh/ccssbxf/img@master/blog/20240428155653.png" alt="image-20240428155645922" style="zoom:50%;" />

<img src="https://testingcf.jsdelivr.net/gh/ccssbxf/img@master/blog/20240428155732.png" alt="image-20240428155732761" style="zoom: 67%;" />

<h3 id="1-自定义函数类"><a href="#1-自定义函数类" class="headerlink" title="1.自定义函数类"></a>1.自定义函数类</h3><p>新建MySumUDAF类，继承AbstractGenericUDAFResolver类，重写两个方法，返回处理类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySumUDAF</span> <span class="keyword">extends</span> <span class="title class_">AbstractGenericUDAFResolver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> GenericUDAFEvaluator <span class="title function_">getEvaluator</span><span class="params">(GenericUDAFParameterInfo info)</span> <span class="keyword">throws</span> SemanticException &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">MySumUDAFEvaluator</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> GenericUDAFEvaluator <span class="title function_">getEvaluator</span><span class="params">(TypeInfo[] info)</span> <span class="keyword">throws</span> SemanticException &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">MySumUDAFEvaluator</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-自定义处理类"><a href="#2-自定义处理类" class="headerlink" title="2.自定义处理类"></a>2.自定义处理类</h3><p>新建MySumUDAFEvaluator类，继承org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator类，重写上述七个方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MySumUDAFEvaluator</span> <span class="keyword">extends</span> <span class="title class_">GenericUDAFEvaluator</span>&#123;&#125;</span><br></pre></td></tr></table></figure>

<p>创建三个变量</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 输入的参数类型</span></span><br><span class="line"><span class="keyword">private</span> PrimitiveObjectInspector in;</span><br><span class="line"><span class="comment">// 输出的参数类型</span></span><br><span class="line"><span class="keyword">private</span> ObjectInspector out;</span><br><span class="line"><span class="comment">// 中间过程数据类型</span></span><br><span class="line"><span class="keyword">private</span> PrimitiveObjectInspector buffer;</span><br></pre></td></tr></table></figure>

<h3 id="3-重写init方法"><a href="#3-重写init方法" class="headerlink" title="3.重写init方法"></a>3.重写init方法</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> ObjectInspector <span class="title function_">init</span><span class="params">(Mode m, ObjectInspector[] parameters)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">    <span class="built_in">super</span>.init(m, parameters);</span><br><span class="line">    <span class="comment">//map阶段读取sql列，输入为String基础数据格式</span></span><br><span class="line">    <span class="keyword">if</span> (Mode.PARTIAL1.equals(m) || Mode.COMPLETE.equals(m)) &#123;</span><br><span class="line">        in = (PrimitiveObjectInspector) parameters[<span class="number">0</span>];</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 其他阶段，处理与聚合结果，输入为Integer基础数据格式</span></span><br><span class="line">        buffer = (PrimitiveObjectInspector) parameters[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//指定输出类型为Integer类型</span></span><br><span class="line">    out = ObjectInspectorFactory.getReflectionObjectInspector(Integer.class, ObjectInspectorFactory.ObjectInspectorOptions.JAVA);</span><br><span class="line">    <span class="keyword">return</span> out;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-重写getNewAggregationBuffer方法"><a href="#4-重写getNewAggregationBuffer方法" class="headerlink" title="4.重写getNewAggregationBuffer方法"></a>4.重写getNewAggregationBuffer方法</h3><p>该方法用于获取缓存类</p>
<p>新建MySumBuffer缓存类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MySumBuffer</span> <span class="keyword">extends</span> <span class="title class_">AbstractAggregationBuffer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">MySumBuffer</span><span class="params">()</span> &#123;</span><br><span class="line">        sum=<span class="number">0L</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> Long sum;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(Long num)</span> &#123;</span><br><span class="line">        sum += num;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getSum</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sum;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reset</span><span class="params">()</span> &#123;</span><br><span class="line">        sum = <span class="number">0L</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重写getNewAggregationBuffer方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> AggregationBuffer <span class="title function_">getNewAggregationBuffer</span><span class="params">()</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">MySumBuffer</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-重写iterate方法"><a href="#5-重写iterate方法" class="headerlink" title="5.重写iterate方法"></a>5.重写iterate方法</h3><p>每行hive数据会调用一次该方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">iterate</span><span class="params">(AggregationBuffer agg, Object[] parameters)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">    <span class="type">Object</span> <span class="variable">obj</span> <span class="operator">=</span> in.getPrimitiveJavaObject(parameters[<span class="number">0</span>]);</span><br><span class="line">    ((MySumBuffer) agg).add(Long.parseLong(obj.toString()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-重写merge方法"><a href="#6-重写merge方法" class="headerlink" title="6.重写merge方法"></a>6.重写merge方法</h3><p>Partial2 阶段和 final 阶段都会调用，聚合 buffer 中的数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 合并缓冲区,对于combine、reduce阶段</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">merge</span><span class="params">(AggregationBuffer agg, Object partial)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">    <span class="keyword">if</span> (Objects.nonNull(partial)) &#123;</span><br><span class="line">        <span class="type">Long</span> <span class="variable">in</span> <span class="operator">=</span> (Long) buffer.getPrimitiveJavaObject(partial);</span><br><span class="line">        ((MySumBuffer) agg).add(in);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="7-重写terminatePartial方法"><a href="#7-重写terminatePartial方法" class="headerlink" title="7.重写terminatePartial方法"></a>7.重写terminatePartial方法</h3><p>预聚合，对于combine阶段。由于这里自定义的是求和函数，因此这里预聚合和最终聚合操作一样</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">terminatePartial</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">    <span class="keyword">return</span> terminate(agg);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-重写terminate方法"><a href="#8-重写terminate方法" class="headerlink" title="8.重写terminate方法"></a>8.重写terminate方法</h3><p>final 阶段调用，会聚合最终结果</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">terminate</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">    <span class="keyword">return</span> ((MySumBuffer) agg).getSum();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="9-重写reset方法"><a href="#9-重写reset方法" class="headerlink" title="9.重写reset方法"></a>9.重写reset方法</h3><p>重置缓存类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reset</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">    ((MySumBuffer) agg).reset();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="10-函数创建使用"><a href="#10-函数创建使用" class="headerlink" title="10.函数创建使用"></a>10.函数创建使用</h3><p>将打好的包上传到hdfs上，例如传到&#x2F;hive&#x2F;lib下</p>
<p>创建函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add</span> jar hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span>hive<span class="operator">/</span>lib<span class="operator">/</span>HiveUDF<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">function</span> mysum <span class="keyword">as</span>  <span class="string">&#x27;com.zhouhj.tools.hive.udaf.MySumUDF&#x27;</span> <span class="keyword">USING</span> JAR <span class="string">&#x27;hdfs:///hive/lib/HiveUDF-1.0-SNAPSHOT.jar&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>使用函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> mysum(a) <span class="keyword">from</span> e3;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line"><span class="number">17</span></span><br></pre></td></tr></table></figure>

<h2 id="UDTF函数编写"><a href="#UDTF函数编写" class="headerlink" title="UDTF函数编写"></a>UDTF函数编写</h2><h3 id="1-编写函数-1"><a href="#1-编写函数-1" class="headerlink" title="1.编写函数"></a>1.编写函数</h3><p>编写mysplit(字段,分隔符)为例，分割字符串</p>
<p>1.继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF类，有三个函数需要重写</p>
<p>2.代码块</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhouhj.tools.hive.udtf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySplitUDTF</span> <span class="keyword">extends</span> <span class="title class_">GenericUDTF</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> StructObjectInspector <span class="title function_">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException &#123;</span><br><span class="line">        List&lt;String&gt; fieldsNameList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">//查询输出数据的默认列名</span></span><br><span class="line">        fieldsNameList.add(<span class="string">&quot;_c0&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//输出的数据类型</span></span><br><span class="line">        List&lt;ObjectInspector&gt; fieldsOutPutOIList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        fieldsOutPutOIList.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//最终返回值,字段名，字段类型</span></span><br><span class="line">        <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldsNameList, fieldsOutPutOIList);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">        <span class="comment">//校验参数个数</span></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">HiveException</span>(<span class="string">&quot;args must be(str,splitKey)&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> args[<span class="number">0</span>].toString();</span><br><span class="line">        <span class="type">String</span> <span class="variable">splitKey</span> <span class="operator">=</span> args[<span class="number">1</span>].toString();</span><br><span class="line">        String[] strArr = str.split(splitKey);</span><br><span class="line">        <span class="comment">//这里一进多出，就像是map或者reduce的写出过程 context.write(key,value)</span></span><br><span class="line">        <span class="comment">//只不过在hive中是forward</span></span><br><span class="line">        <span class="keyword">for</span> (String s : strArr) &#123;</span><br><span class="line">            forward(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-函数创建使用-1"><a href="#2-函数创建使用-1" class="headerlink" title="2.函数创建使用"></a>2.函数创建使用</h3><p>将打好的包上传到hdfs上，例如传到&#x2F;hive&#x2F;lib下</p>
<p>创建函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add</span> jar hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span>hive<span class="operator">/</span>lib<span class="operator">/</span>HiveUDF<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">function</span> mysplitas <span class="string">&#x27;com.zhouhj.tools.hive.udf.MySplitUDTF&#x27;</span> <span class="keyword">USING</span> JAR <span class="string">&#x27;hdfs:///hive/lib/HiveUDF-1.0-SNAPSHOT.jar&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>使用函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> mysplit(<span class="string">&#x27;a;b;c&#x27;</span>,<span class="string">&#x27;;&#x27;</span>);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">c</span><br></pre></td></tr></table></figure>

<h2 id="UDF管理"><a href="#UDF管理" class="headerlink" title="UDF管理"></a>UDF管理</h2><p>以下操作如果 hiveserver2 是使用的 vip，则需要使用 beeline 连接每台 hiveserver2 上去，都执行一遍。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">添加jar包（推荐使用hdfs方式，本地包也可以，但是麻烦）如果自定义函数里有使用到第三方的jar包，也需要使用这种方式添加到hive里</span><br><span class="line"><span class="keyword">add</span> jar hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span>hive<span class="operator">/</span>lib<span class="operator">/</span>HiveUDF<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"></span><br><span class="line">创建函数(如果是在beeline连接创建的，则只有beeline连接的那台hiveserver2有效，其他的无效)</span><br><span class="line">如果是在hive shell执行，则对hive集群的所有hive shell都生效，对beeline无效。hive集群重启之后，对beeline也有效</span><br><span class="line"><span class="keyword">create</span> [temporary] <span class="keyword">function</span> mysum <span class="keyword">as</span>  <span class="string">&#x27;com.zhouhj.tools.hive.udaf.MySumUDF&#x27;</span> [<span class="keyword">USING</span> JAR <span class="string">&#x27;hdfs:///hive/lib/HiveUDF-1.0-SNAPSHOT.jar&#x27;</span>];</span><br><span class="line"></span><br><span class="line">删除函数(需要在所有 hiveserver2 节点删除，只在一个节点上操作，用 beeline 连接其他节点 UDF 函数还会存在。或者只 beeline 一个 hiveserver2 删除，重启所有 hiveserver2 后，生效)</span><br><span class="line"><span class="keyword">drop</span> [temporary] <span class="keyword">function</span> [if <span class="keyword">exists</span>] mysum;</span><br><span class="line"></span><br><span class="line">删除jar包</span><br><span class="line"><span class="keyword">delete</span> jar hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span>hive<span class="operator">/</span>lib<span class="operator">/</span>HiveUDF<span class="number">-1.0</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"></span><br><span class="line">更新UDF函数的话需要先走删除UDF函数、删除jar包、再新增jar包、新增函数的方式</span><br></pre></td></tr></table></figure>

<p>参考：</p>
<p><a href="https://blog.csdn.net/weixin_46429290/article/details/126634429">https://blog.csdn.net/weixin_46429290/article/details/126634429</a></p>
<p><a href="https://blog.csdn.net/nmsLLCSDN/article/details/125833600">https://blog.csdn.net/nmsLLCSDN/article/details/125833600</a></p>
<p><a href="https://mp.weixin.qq.com/s/9jGlRDfTW_G4TV7WHmHMdw">https://mp.weixin.qq.com/s/9jGlRDfTW_G4TV7WHmHMdw</a></p>
<p><a href="https://mp.weixin.qq.com/s/s3d2VGT_vQvZ12bNoIWNOg">https://mp.weixin.qq.com/s/s3d2VGT_vQvZ12bNoIWNOg</a></p>
<p><a href="https://blog.csdn.net/zyz_home/article/details/79889519">https://blog.csdn.net/zyz_home/article/details/79889519</a></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>UDF</tag>
        <tag>Hive</tag>
        <tag>UDAF</tag>
        <tag>UDTF</tag>
      </tags>
  </entry>
</search>
